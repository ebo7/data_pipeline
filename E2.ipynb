{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFm792v4E/1PxO8x/EcgPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebo7/data_pipeline/blob/main/E2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Nlqvcz1RiH"
      },
      "source": [
        "# Automating Weekly Data Ingenstion from GHR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6AltxkYo7Wy"
      },
      "source": [
        "# TASK 1 & 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrCkwW6u-iRO"
      },
      "source": [
        "## Resources\n",
        "Following resources were used to emulate the problem setup:\n",
        "1. AWS Transfer Family - SFTP Server,\n",
        " * server id: s-1dab728b35294a138 \\\\\n",
        " * server hosts weekly data on s3://dentsu-labs-demo/sftp \n",
        "2. Persistent EBS Volume\n",
        " * Volume ID: vol-0fbb4f932e366ade5\n",
        "3. AWS Data Pipeline \n",
        "4. S3 bucket\n",
        " * S3 URI: s3://dentsu-labs-demo \\\\\n",
        " * ADP ingests data from s3://dentsu-labs-demo/new \\\\\n",
        " * After ADP transfers the data to Redshift, it is moved to s3://dentsu-labs-demo/old, which stores all previous data\n",
        "5. Redshift cluster - Cluster ID: dentsu-labs-redshift-cluster\n",
        "\n",
        "(ec2 instance was automatically started and terminated by the AWS Data Pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hkC_u2_54F"
      },
      "source": [
        "## Overview\n",
        "A. Manually create tables and view on Redshift\n",
        "\n",
        "B. AWS Data Pipeline (After step A)\n",
        "1. Start EC2\n",
        "2. Attach and Mount Persistent Elastic Block Store (EBS), which stores private key for sftp\n",
        "3. Mount GHR directory on EC2 using sshfs\n",
        "4. Copy directories that were modified within the past week to s3://dentsu-labs-demo/new/\n",
        "5. Copy the new data to Redshift\n",
        "\n",
        "For the first week only, step B script is modified to include all directories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmxgm4j8jO6m"
      },
      "source": [
        "# Step A: Create tables, view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_uj_6puoOuB"
      },
      "source": [
        "# master\n",
        "CREATE TABLE master(job_id CHAR(9) PRIMARY KEY DISTKEY SORTKEY , company VARCHAR(150), post_date TIMESTAMP, salary INTEGER, city VARCHAR(100))\n",
        "# title\n",
        "CREATE TABLE title(job_id CHAR(9) PRIMARY KEY DISTKEY SORTKEY , title VARCHAR(100))\n",
        "# timelog\n",
        "CREATE TABLE timelog(job_id CHAR(9) PRIMARY KEY DISTKEY SORTKEY , remove_date TIMESTAMP)\n",
        "\n",
        "# view\n",
        "# view was assumed to be based on master\n",
        "CREATE VIEW posting_current AS\n",
        "(\n",
        "  SELECT master.job_id, city, company, title, salary, post_date, remove_date\n",
        "  FROM master\n",
        "  LEFT JOIN title on master.job_id = title.job_id\n",
        "  LEFT JOIN timelog on master.job_id = timelog.job_id\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5_a8PDyqMwK"
      },
      "source": [
        "## Step B: AWS Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkcgN3HQOU_A"
      },
      "source": [
        "![alt](https://drive.google.com/uc?export=view&id=1pCwRuAzOddvlaCBRbL9my_8kGGs_osGk)\n",
        "(Detailed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWWkUCSQPO-z"
      },
      "source": [
        "![alt](https://drive.google.com/uc?export=view&id=1WEl0rtSzfBOrWsYyaamRKvj1TK1Oymnx)\n",
        "(Brief) \\\\\n",
        ">* Shell 1: Copy new data from GHR to dentsu Labs S3\n",
        "* Redshift: Enter data into tables\n",
        "* Shell 2: Cleanup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JeM6-OWqoQ2"
      },
      "source": [
        "### Shell 1 (Amazon Linux)\n",
        "Default ec2 for ADP is Amazon Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6o_PtzY1PPX"
      },
      "source": [
        "# Install sshfs\n",
        "sudo yum-config-manager --enable epel\n",
        "sudo yum -y install sshfs\n",
        "\n",
        "\n",
        "# Attach EBS Volume\n",
        "aws configure set default.region us-east-1\n",
        "INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id/)\n",
        "VOL_ID=vol-0fbb4f932e366ade5\n",
        "aws ec2 attach-volume --device /dev/sdf --volume-id $VOL_ID --instance-id $INSTANCE_ID\n",
        "# Block until attachment is finished\n",
        "while [ ! -b $(readlink -f /dev/xvdf) ]\n",
        "  do echo \"waiting for device /dev/xvdf\" \n",
        "  sleep 5\n",
        "done\n",
        "# Mount EBS Volume\n",
        "sudo mkdir /root/ebs-mnt\n",
        "sudo mount /dev/sdf /root/ebs-mnt\n",
        "\n",
        "\n",
        "# Mount GHR directory\n",
        "USER=demo\n",
        "ID_PATH=/root/ebs-mnt/private-key\n",
        "SERVER=s-1dab728b35294a138.server.transfer.us-east-1.amazonaws.com:\n",
        "mkdir ghr-mnt\n",
        "sudo chmod 400 $ID_PATH\n",
        "#install sshfs\n",
        "# sudo apt-get update\n",
        "# sudo apt-get install sshfs\n",
        "sudo sshfs -o StrictHostKeyChecking=no,allow_other,IdentityFile=$ID_PATH $USER@$SERVER ./ghr-mnt\n",
        "\n",
        "\n",
        "# Copy new directories to s3/old; keep the GHR directory structure\n",
        "BUCKET=s3://dentsu-labs-demo\n",
        "cd ghr-mnt\n",
        "find . -mindepth 1 -type d -mtime -7 | cut -c3- | while read line; do aws s3 cp --recursive $line $BUCKET/old/$line; done\n",
        "\n",
        "\n",
        "# Copy new directories to S3/new, which is ingested by ADP. Directory structure is modified for ADP\n",
        "find . -mtime -7 -type f -regex '\\./.*/master.*' | while read line; do aws s3 cp $line $BUCKET/new/master/; done\n",
        "find . -mtime -7 -type f -regex '\\./.*/timelog.*' | while read line; do aws s3 cp $line $BUCKET/new/timelog/; done\n",
        "find . -mtime -7 -type f -regex '\\./.*/title.*' | while read line; do aws s3 cp $line $BUCKET/new/title/; done\n",
        "\n",
        "# IMPROVE: Copy directories only once to s3/new and after ADP ingestion move to s3/old "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU7nmAaV_-DR"
      },
      "source": [
        "### Redshift Copy\n",
        "Redshift Copy takes:\n",
        "* input(S3) & output (Redshift table) \\\\\n",
        "* command \\\\\n",
        "* insert mode \\\\\n",
        "\n",
        "Redshift Copy is handled by ADP and the details can be found in JSON at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcv7tYhoRic7"
      },
      "source": [
        "### Shell Command 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7vc5wS0QwCW"
      },
      "source": [
        "# clean up\n",
        "BUCKET=s3://dentsu-labs-demo\n",
        "INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id/)\n",
        "VOL_ID=vol-0fbb4f932e366ade5\n",
        "# remove new files\n",
        "aws s3 rm --recursive $BUCKET/new\n",
        "cd ..\n",
        "# unmount ghr and ebs\n",
        "sudo umount ghr-mnt\n",
        "sudo umount /root/ebs-mnt\n",
        "aws ec2 detach-volume --device /dev/sdf --volume-id $VOL_ID --instance-id $INSTANCE_ID\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPUfUvhCfpi2"
      },
      "source": [
        "## Choosing between AWS Services\n",
        "* Running ADP would start ec2 instances on demand weekly and could reduce cost\n",
        "* With temporary ec2, private-key is stored outside the ec2's instance storage and 2 exterior stroage options can be s3 and s3:\n",
        " 1. Option 1: Copy from S3: Repeatedly copying private-key from s3 to ec2 might have security risks. \n",
        " 2. Option 2: Mount s3 virtually with s3fs: ghr directory is being mounted with sshfs and requires IdentityFile. There was an issue with providing private-key from a mounted s3 filesystem and needed more exploration to debug. \n",
        " 3. Option 3: IdentityFile on persistent ebs volume was compatible for mounting ghr directory on ec2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY7OLFiVm7Fs"
      },
      "source": [
        "# TASK 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EQsHJuYm-Lm"
      },
      "source": [
        "# create temporary table listing all months as int\n",
        "CREATE OR REPLACE PROCEDURE p1()AS $$\n",
        "DECLARE\n",
        "  loop_var int;\n",
        "  loop_end int:=CAST(FLOOR(MONTHS_BETWEEN(GETDATE(), '2014-01-01')) AS INTEGER);\n",
        "BEGIN\n",
        "  DROP TABLE if exists months;\n",
        "  CREATE  TEMP TABLE months(month int);\n",
        "    FOR loop_var IN 0..loop_end LOOP\n",
        "        insert into months values(CAST(SUBSTRING(DATEADD(month, loop_var,'2014-01-01'),1,4) + SUBSTRING(DATEADD(month, loop_var,'2014-01-01'), 6,2) AS INTEGER));\n",
        "    END LOOP;\n",
        "END;\n",
        "$$ LANGUAGE plpgsql;\n",
        "\n",
        "CALL p1();\n",
        "\n",
        "# checked if the output value is 0 or null and value that is 0 and not null is displayed as empty on redshift.\n",
        "# can force the int 0 to show as char '0' when sum=0 and sum!=null \n",
        "SELECT \n",
        "\tSUBSTRING(CAST(month AS CHAR(6)),1,4)+'-'+SUBSTRING(CAST(month AS CHAR(6)),5,2) AS month,\n",
        "\tSUM(is_new) count_new,\n",
        "  SUM(is_removed) count_removed,\n",
        "  SUM(is_active) count_active,\n",
        "  AVG(salary_new) salary_new,\n",
        "  AVG(salary_removed) salary_removed,\n",
        "  AVG(salary_active) salary_active\n",
        "FROM\n",
        "(\n",
        "  SELECT \n",
        "      month,\n",
        "      post_date,\n",
        "      CASE WHEN CAST(SUBSTRING(post_date,1,4) + SUBSTRING(post_date, 6,2) AS INTEGER)=month \n",
        "                THEN 1\n",
        "                ELSE 0 \n",
        "      END AS is_new,\n",
        "      CASE WHEN CAST(SUBSTRING(remove_date,1,4) + SUBSTRING(remove_date, 6,2) AS INTEGER)=month \n",
        "                THEN 1\n",
        "                ELSE 0 \n",
        "      END AS is_removed,\n",
        "      CASE WHEN (CAST(SUBSTRING(remove_date,1,4) + SUBSTRING(remove_date, 6,2) AS INTEGER)>=month AND CAST(SUBSTRING(post_date,1,4) + SUBSTRING(post_date, 6,2) AS INTEGER)<=month) \n",
        "                THEN 1\n",
        "                ELSE 0 \n",
        "      END AS is_active,\n",
        "      CASE WHEN is_new=1\n",
        "          THEN salary\n",
        "          ELSE null\n",
        "      END AS salary_new,\n",
        "      CASE WHEN is_removed=1\n",
        "          THEN salary\n",
        "          ELSE null\n",
        "      END AS salary_removed,\n",
        "      CASE WHEN is_active=1\n",
        "          THEN salary\n",
        "          ELSE null\n",
        "      END AS salary_active\n",
        "  FROM months, posting_current\n",
        ")\n",
        "GROUP BY month\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YBYzFGjeV1b"
      },
      "source": [
        "## ADP JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEPm-Dmt-ATb"
      },
      "source": [
        "{\n",
        "  \"objects\": [\n",
        "    {\n",
        "      \"directoryPath\": \"s3://dentsu-labs-demo/new/timelog\",\n",
        "      \"name\": \"s3 - timelog\",\n",
        "      \"id\": \"DataNodeId_h710s\",\n",
        "      \"type\": \"S3DataNode\"\n",
        "    },\n",
        "    {\n",
        "      \"resourceRole\": \"DataPipelineDefaultResourceRole\",\n",
        "      \"role\": \"DataPipelineDefaultRole\",\n",
        "      \"name\": \"ec2\",\n",
        "      \"id\": \"ResourceId_bc0wq\",\n",
        "      \"type\": \"Ec2Resource\",\n",
        "      \"availabilityZone\": \"us-east-1b\"\n",
        "    },\n",
        "    {\n",
        "      \"database\": {\n",
        "        \"ref\": \"RedshiftDatabaseId_mT4AA\"\n",
        "      },\n",
        "      \"name\": \"table - timelog\",\n",
        "      \"id\": \"DataNodeId_TFhdv\",\n",
        "      \"type\": \"RedshiftDataNode\",\n",
        "      \"tableName\": \"timelog\"\n",
        "    },\n",
        "    {\n",
        "      \"output\": {\n",
        "        \"ref\": \"DataNodeId_cWFVd\"\n",
        "      },\n",
        "      \"input\": {\n",
        "        \"ref\": \"DataNodeId_spAxU\"\n",
        "      },\n",
        "      \"dependsOn\": {\n",
        "        \"ref\": \"ShellCommandActivityId_RCZNF\"\n",
        "      },\n",
        "      \"name\": \"Redshift Copy: title\",\n",
        "      \"commandOptions\": \"csv\\nignoreheader 1\\nACCEPTINVCHARS\\nTRUNCATECOLUMNS\",\n",
        "      \"runsOn\": {\n",
        "        \"ref\": \"ResourceId_bc0wq\"\n",
        "      },\n",
        "      \"id\": \"RedshiftCopyActivityId_rBhBZ\",\n",
        "      \"type\": \"RedshiftCopyActivity\",\n",
        "      \"insertMode\": \"OVERWRITE_EXISTING\"\n",
        "    },\n",
        "    {\n",
        "      \"database\": {\n",
        "        \"ref\": \"RedshiftDatabaseId_mT4AA\"\n",
        "      },\n",
        "      \"name\": \"table - master\",\n",
        "      \"id\": \"DataNodeId_3KDbt\",\n",
        "      \"type\": \"RedshiftDataNode\",\n",
        "      \"tableName\": \"master\"\n",
        "    },\n",
        "    {\n",
        "      \"failureAndRerunMode\": \"CASCADE\",\n",
        "      \"resourceRole\": \"DataPipelineDefaultResourceRole\",\n",
        "      \"role\": \"DataPipelineDefaultRole\",\n",
        "      \"scheduleType\": \"ONDEMAND\",\n",
        "      \"name\": \"Default\",\n",
        "      \"id\": \"Default\"\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"Shell 1\",\n",
        "      \"runsOn\": {\n",
        "        \"ref\": \"ResourceId_bc0wq\"\n",
        "      },\n",
        "      \"id\": \"ShellCommandActivityId_RCZNF\",\n",
        "      \"type\": \"ShellCommandActivity\",\n",
        "      \"command\": \"# Install sshfs\\nsudo yum-config-manager --enable epel\\nsudo yum -y install sshfs\\n\\n\\n# Attach EBS Volume\\naws configure set default.region us-east-1\\nINSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id/)\\nVOL_ID=vol-0fbb4f932e366ade5\\naws ec2 attach-volume --device /dev/sdf --volume-id $VOL_ID --instance-id $INSTANCE_ID\\n# Block until attachment is finished\\nwhile [ ! -b $(readlink -f /dev/xvdf) ]\\n  do echo \\\"waiting for device /dev/xvdf\\\" \\n  sleep 5\\ndone\\n# Mount EBS Volume\\nsudo mkdir /root/ebs-mnt\\nsudo mount /dev/sdf /root/ebs-mnt\\n\\n\\n# Mount GHR directory\\nUSER=demo\\nID_PATH=/root/ebs-mnt/private-key\\nSERVER=s-1dab728b35294a138.server.transfer.us-east-1.amazonaws.com:\\nmkdir ghr-mnt\\nsudo chmod 400 $ID_PATH\\n#install sshfs\\n# sudo apt-get update\\n# sudo apt-get install sshfs\\nsudo sshfs -o StrictHostKeyChecking=no,allow_other,IdentityFile=$ID_PATH $USER@$SERVER ./ghr-mnt\\n\\n\\n# Copy new directories to s3/old; keep the GHR directory structure\\nBUCKET=s3://dentsu-labs-demo\\ncd ghr-mnt\\nfind . -mindepth 1 -type d -mtime -7 | cut -c3- | while read line; do aws s3 cp --recursive $line $BUCKET/old/$line; done\\n\\n\\n# Copy new directories to S3/new, which is ingested by ADP. Directory structure is modified for ADP\\nfind . -mtime -7 -type f -regex '\\\\./.*/master.*' | while read line; do aws s3 cp $line $BUCKET/new/master/; done\\nfind . -mtime -7 -type f -regex '\\\\./.*/timelog.*' | while read line; do aws s3 cp $line $BUCKET/new/timelog/; done\\nfind . -mtime -7 -type f -regex '\\\\./.*/title.*' | while read line; do aws s3 cp $line $BUCKET/new/title/; done\\n\\n# IMPROVE: Copy directories only once to s3/new and after ADP ingestion move to s3/old \"\n",
        "    },\n",
        "    {\n",
        "      \"*password\": \"dentsu-labs-admin1\",\n",
        "      \"name\": \"DefaultRedshiftDatabase1\",\n",
        "      \"id\": \"RedshiftDatabaseId_mT4AA\",\n",
        "      \"clusterId\": \"dentsu-labs-redshift-cluster\",\n",
        "      \"type\": \"RedshiftDatabase\",\n",
        "      \"username\": \"dentsu-labs-admin\"\n",
        "    },\n",
        "    {\n",
        "      \"output\": {\n",
        "        \"ref\": \"DataNodeId_TFhdv\"\n",
        "      },\n",
        "      \"input\": {\n",
        "        \"ref\": \"DataNodeId_h710s\"\n",
        "      },\n",
        "      \"dependsOn\": {\n",
        "        \"ref\": \"ShellCommandActivityId_RCZNF\"\n",
        "      },\n",
        "      \"name\": \"Redshift Copy: timelog\",\n",
        "      \"commandOptions\": \"csv\\nignoreheader 1\\nACCEPTINVCHARS\\nTRUNCATECOLUMNS\",\n",
        "      \"runsOn\": {\n",
        "        \"ref\": \"ResourceId_bc0wq\"\n",
        "      },\n",
        "      \"id\": \"RedshiftCopyActivityId_yRtTA\",\n",
        "      \"type\": \"RedshiftCopyActivity\",\n",
        "      \"insertMode\": \"OVERWRITE_EXISTING\"\n",
        "    },\n",
        "    {\n",
        "      \"directoryPath\": \"s3://dentsu-labs-demo/new/master\",\n",
        "      \"name\": \"s3 - master\",\n",
        "      \"id\": \"DataNodeId_SZs4z\",\n",
        "      \"type\": \"S3DataNode\"\n",
        "    },\n",
        "    {\n",
        "      \"dependsOn\": [\n",
        "        {\n",
        "          \"ref\": \"RedshiftCopyActivityId_XbujI\"\n",
        "        },\n",
        "        {\n",
        "          \"ref\": \"RedshiftCopyActivityId_rBhBZ\"\n",
        "        },\n",
        "        {\n",
        "          \"ref\": \"RedshiftCopyActivityId_yRtTA\"\n",
        "        }\n",
        "      ],\n",
        "      \"name\": \"Shell 2\",\n",
        "      \"runsOn\": {\n",
        "        \"ref\": \"ResourceId_bc0wq\"\n",
        "      },\n",
        "      \"id\": \"ShellCommandActivityId_OyvMn\",\n",
        "      \"type\": \"ShellCommandActivity\",\n",
        "      \"command\": \"# clean up\\nBUCKET=s3://dentsu-labs-demo\\nINSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id/)\\nVOL_ID=vol-0fbb4f932e366ade5\\naws s3 rm --recursive $BUCKET/new\\ncd ..\\nsudo umount ghr-mnt\\nsudo umount /root/ebs-mnt\\naws ec2 detach-volume --device /dev/sdf --volume-id $VOL_ID --instance-id $INSTANCE_ID\\n\\n\"\n",
        "    },\n",
        "    {\n",
        "      \"database\": {\n",
        "        \"ref\": \"RedshiftDatabaseId_mT4AA\"\n",
        "      },\n",
        "      \"name\": \"table - title\",\n",
        "      \"id\": \"DataNodeId_cWFVd\",\n",
        "      \"type\": \"RedshiftDataNode\",\n",
        "      \"tableName\": \"title\"\n",
        "    },\n",
        "    {\n",
        "      \"directoryPath\": \"s3://dentsu-labs-demo/new/title\",\n",
        "      \"name\": \"s3 - title\",\n",
        "      \"id\": \"DataNodeId_spAxU\",\n",
        "      \"type\": \"S3DataNode\"\n",
        "    },\n",
        "    {\n",
        "      \"output\": {\n",
        "        \"ref\": \"DataNodeId_3KDbt\"\n",
        "      },\n",
        "      \"input\": {\n",
        "        \"ref\": \"DataNodeId_SZs4z\"\n",
        "      },\n",
        "      \"dependsOn\": {\n",
        "        \"ref\": \"ShellCommandActivityId_RCZNF\"\n",
        "      },\n",
        "      \"name\": \"Redshift Copy: master\",\n",
        "      \"commandOptions\": \"csv\\nignoreheader 1\\nACCEPTINVCHARS\\nTRUNCATECOLUMNS\",\n",
        "      \"runsOn\": {\n",
        "        \"ref\": \"ResourceId_bc0wq\"\n",
        "      },\n",
        "      \"id\": \"RedshiftCopyActivityId_XbujI\",\n",
        "      \"type\": \"RedshiftCopyActivity\",\n",
        "      \"insertMode\": \"OVERWRITE_EXISTING\"\n",
        "    }\n",
        "  ],\n",
        "  \"parameters\": []\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
